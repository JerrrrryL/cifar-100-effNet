{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "resnet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOQ8qv9oiXYa80+K6TnmXkd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JerrrrryL/cifar-100-effNet/blob/master/resnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jvfj24EUWZGf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ec4718a-e63d-4e92-a2b7-7b89500e717f"
      },
      "source": [
        "! git clone https://github.com/davda54/sam.git\n",
        "! pip install opacus\n",
        "! git clone https://github.com/hyhmia/BlindMI.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'sam' already exists and is not an empty directory.\n",
            "Requirement already satisfied: opacus in /usr/local/lib/python3.7/dist-packages (0.13.0)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from opacus) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.40 in /usr/local/lib/python3.7/dist-packages (from opacus) (4.41.1)\n",
            "Requirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.7/dist-packages (from opacus) (0.9.1+cu101)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.7/dist-packages (from opacus) (1.8.1+cu101)\n",
            "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.7/dist-packages (from opacus) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.25.1 in /usr/local/lib/python3.7/dist-packages (from opacus) (2.25.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.9->opacus) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3->opacus) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->opacus) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->opacus) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->opacus) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->opacus) (1.24.3)\n",
            "fatal: destination path 'BlindMI' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhJ8k3BkBI-T"
      },
      "source": [
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "\n",
        "__all__ = ['ResNet', 'resnet50', 'resnet101']\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"3x3 convolution with padding\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.GroupNorm(32, planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.GroupNorm(32, planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.GroupNorm(32, planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "        gn_init(self.bn1)\n",
        "        gn_init(self.bn2)\n",
        "        gn_init(self.bn3, zero_init=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "def conv2d_init(m):\n",
        "    assert isinstance(m, nn.Conv2d)\n",
        "    n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "    m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "\n",
        "def gn_init(m, zero_init=False):\n",
        "    assert isinstance(m, nn.GroupNorm)\n",
        "    m.weight.data.fill_(0. if zero_init else 1.)\n",
        "    m.bias.data.zero_()\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000):\n",
        "        self.inplanes = 64\n",
        "        super(ResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.GroupNorm(32, 64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                conv2d_init(m)\n",
        "        gn_init(self.bn1)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.GroupNorm(32, planes * block.expansion),\n",
        "            )\n",
        "            m = downsample[1]\n",
        "            assert isinstance(m, nn.GroupNorm)\n",
        "            gn_init(m)\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def resnet50(**kwargs):\n",
        "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
        "    return model\n",
        "\n",
        "def resnet101(**kwargs):\n",
        "    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
        "    return model"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IU3R9K-edArM"
      },
      "source": [
        "import torch\n",
        "\n",
        "import sys\n",
        "import torch.nn as nn\n",
        "sys.path.append('sam/example/')\n",
        "from utility.cutout import Cutout\n",
        "from model.wide_res_net import WideResNet\n",
        "from model.smooth_cross_entropy import smooth_crossentropy\n",
        "from utility.log import Log\n",
        "from utility.initialize import initialize\n",
        "from utility.step_lr import StepLR\n",
        "from opacus import PrivacyEngine\n",
        "import opacus.utils.module_modification as module_modification\n",
        "from opacus.dp_model_inspector import DPModelInspector\n",
        "from torchvision.models import wide_resnet101_2\n",
        "\n",
        "class SAM(torch.optim.Optimizer):\n",
        "    def __init__(self, params, base_optimizer, rho=0.05, **kwargs):\n",
        "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
        "\n",
        "        defaults = dict(rho=rho, **kwargs)\n",
        "        super(SAM, self).__init__(params, defaults)\n",
        "\n",
        "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
        "        self.param_groups = self.base_optimizer.param_groups\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def first_step(self, zero_grad=False):\n",
        "        grad_norm = self._grad_norm()\n",
        "        for group in self.param_groups:\n",
        "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
        "\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None: continue\n",
        "                e_w = p.grad * scale.to(p)\n",
        "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
        "                self.state[p][\"e_w\"] = e_w\n",
        "\n",
        "        if zero_grad: self.zero_grad()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def second_step(self, zero_grad=False):\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None: continue\n",
        "                p.sub_(self.state[p][\"e_w\"])  # get back to \"w\" from \"w + e(w)\"\n",
        "\n",
        "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
        "\n",
        "        if zero_grad: self.zero_grad()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
        "        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n",
        "\n",
        "        self.first_step(zero_grad=True)\n",
        "        closure()\n",
        "        self.second_step()\n",
        "\n",
        "    def _grad_norm(self):\n",
        "        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
        "        norm = torch.norm(\n",
        "                    torch.stack([\n",
        "                        p.grad.norm(p=2).to(shared_device)\n",
        "                        for group in self.param_groups for p in group[\"params\"]\n",
        "                        if p.grad is not None\n",
        "                    ]),\n",
        "                    p=2\n",
        "               )\n",
        "        return norm\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63wV22sm3y_4"
      },
      "source": [
        "import torch\n",
        "import os\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections import OrderedDict\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from PIL.Image import BICUBIC\n",
        "from opacus.utils.uniform_sampler import UniformWithReplacementSampler\n",
        "import torchvision.models as models\n",
        "\n",
        "dropout = 0.2\n",
        "learning_rate = 8e-4\n",
        "label_smoothing = 0.001\n",
        "momentum = 0.9\n",
        "threads = 2\n",
        "rho = 0.05\n",
        "weight_decay = 0.0005\n",
        "noise_multiplier = 0.005\n",
        "DELTA = 1e-5\n",
        "class_num = 100\n",
        "MODEL_PATH = 'target_models/'\n",
        "DATASET = 'cifar-100/'\n",
        "if not os.path.exists(MODEL_PATH + DATASET):\n",
        "      os.makedirs(MODEL_PATH + DATASET)\n",
        "\n",
        "# def get_statistics(class_num):\n",
        "#         if class_num == 10:\n",
        "#             train_set = torchvision.datasets.CIFAR10(root='./cifar', train=True, download=True,\n",
        "#                                                      transform=transforms.ToTensor())\n",
        "#             test_set = torchvision.datasets.CIFAR10(root='./cifar', train=False, download=True,\n",
        "#                                                     transform=transforms.ToTensor())\n",
        "#         else:\n",
        "#             train_set = torchvision.datasets.CIFAR100(root='./cifar', train=True, download=True,\n",
        "#                                                       transform=transforms.ToTensor())\n",
        "#             test_set = torchvision.datasets.CIFAR100(root='./cifar', train=False, download=True,\n",
        "#                                                      transform=transforms.ToTensor())\n",
        "\n",
        "#         data = torch.cat([d[0] for d in DataLoader(train_set)] + [d[0] for d in DataLoader(test_set)])\n",
        "#         return data.mean(dim=[0, 2, 3]), data.std(dim=[0, 2, 3])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9MjgxsrVTDr"
      },
      "source": [
        "def split_dataset(dataset, num_splits, batch_size):\n",
        "  train_size = len(dataset) // num_splits\n",
        "  sample_rate = batch_size / train_size\n",
        "  split_datasets = []\n",
        "  x_remaining = dataset\n",
        "  for i in range(num_splits-1):\n",
        "    x_train, x_remaining = torch.utils.data.random_split(x_remaining, [train_size, len(x_remaining)-train_size])\n",
        "    split_datasets.append(torch.utils.data.DataLoader(x_train, \n",
        "                                                      batch_size=batch_size,\n",
        "                                                      num_workers=threads))\n",
        "  split_datasets.append(torch.utils.data.DataLoader(x_remaining, \n",
        "                                                    batch_size=batch_size,\n",
        "                                                    num_workers=threads))\n",
        "  return split_datasets\n",
        "\n",
        "def train_target_model(train_set, test_set, run, class_num=100, learning_rate=0.1, batch_size=64, virtual_batch_size=256,\n",
        "                       noise_multiplier = 0.002, max_grad_norm=10, epochs=15, dropout=0, weight_decay=0.0005, \n",
        "                       dp=False, pretrained=False, SGD=True):\n",
        "  torch.cuda.empty_cache()\n",
        "  n_acc_steps = virtual_batch_size / batch_size\n",
        "  sample_rate = batch_size / len(train_set)\n",
        "  if pretrained:\n",
        "    model = None\n",
        "    model = resnet50()\n",
        "    state_dict = torch.load(\"ImageNet-ResNet50-GN.pth\")[\"state_dict\"]\n",
        "    # create new OrderedDict that does not contain `module.`\n",
        "    new_state_dict = OrderedDict()\n",
        "    for k, v in state_dict.items():\n",
        "        name = k[7:] # remove `module.`\n",
        "        new_state_dict[name] = v\n",
        "    model.load_state_dict(new_state_dict)\n",
        "    num_ftrs = model.fc.in_features\n",
        "    model.fc = nn.Sequential(\n",
        "        nn.Dropout(dropout),\n",
        "        nn.Linear(num_ftrs, class_num)\n",
        "    )\n",
        "  else:\n",
        "    model = torchvision.models.AlexNet(num_classes=class_num)\n",
        "\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  # print(model)\n",
        "  model = model.to(device)\n",
        "  log = Log(log_each=10)\n",
        "\n",
        "  privacy_engine = PrivacyEngine(\n",
        "      model,\n",
        "      sample_rate=sample_rate,\n",
        "      alphas=[1 + x / 10.0 for x in range(1, 100)] + list(range(12, 64)),\n",
        "      noise_multiplier=noise_multiplier,\n",
        "      max_grad_norm=max_grad_norm,\n",
        "  )\n",
        "  # DP-SGD\n",
        "  if SGD:\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
        "    if dp:\n",
        "      privacy_engine.attach(optimizer)\n",
        "  # SAM\n",
        "  else:\n",
        "    base_optimizer = torch.optim.SGD\n",
        "    if dp:\n",
        "      privacy_engine.attach(base_optimizer)\n",
        "    optimizer = SAM(model.parameters(), base_optimizer, rho=rho, lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
        "  scheduler = StepLR(optimizer, learning_rate, epochs)\n",
        "  for epoch in range(epochs):\n",
        "      model.train()\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "      log.train(len_dataset=len(train_set))\n",
        "      \n",
        "      for batch_ind, (inputs, targets) in enumerate(train_set):\n",
        "          inputs, targets = inputs.to(device), targets.to(device) \n",
        "\n",
        "          # # Uncomment the following line for SAM\n",
        "          # # first forward-backward step\n",
        "          if SGD:\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(inputs)\n",
        "            loss = criterion(predictions, targets)\n",
        "            loss.backward()\n",
        "            # SGD\n",
        "            if dp:\n",
        "              if ((batch_ind + 1) % n_acc_steps == 0) or ((batch_ind + 1) == len(train_set)):\n",
        "                  optimizer.step()\n",
        "              else:\n",
        "                  # accumulate per-example gradients but don't take a step yet\n",
        "                  optimizer.virtual_step()\n",
        "            else:\n",
        "              optimizer.step()\n",
        "          else:\n",
        "            predictions = model(inputs)\n",
        "            loss = smooth_crossentropy(predictions, targets)\n",
        "            loss.mean().backward()\n",
        "\n",
        "            # Uncomment the following for SAM\n",
        "            optimizer.first_step(zero_grad=True)\n",
        "\n",
        "            # second forward-backward step\n",
        "            smooth_crossentropy(model(inputs), targets).mean().backward()\n",
        "            optimizer.second_step(zero_grad=True)\n",
        "\n",
        "          with torch.no_grad():\n",
        "              correct = torch.argmax(predictions.data, 1) == targets\n",
        "              # print(correct.size(0))\n",
        "              # print(torch.cat(batch_size * [torch.reshape(loss.cpu(), (-1,))]))\n",
        "\n",
        "              if SGD:\n",
        "                log(model, \n",
        "                    torch.cat(batch_size * [torch.reshape(loss.cpu(), (-1,))]), \n",
        "                    correct.cpu(), \n",
        "                    scheduler.lr())\n",
        "              else:\n",
        "                log(model, loss.cpu(), correct.cpu(), scheduler.lr())\n",
        "              scheduler(epoch)\n",
        "      # rdp_sgd = get_renyi_divergence(privacy_engine.sample_rate, \n",
        "      #                                privacy_engine.noise_multiplier) * privacy_engine.steps\n",
        "      # epsilon, _ = get_privacy_spent(rdp_sgd)\n",
        "      # print(f\"ε = {epsilon:.3f}\")\n",
        "\n",
        "      model.eval()\n",
        "      log.eval(len_dataset=len(test_set))\n",
        "\n",
        "      with torch.no_grad():\n",
        "          for batch in test_set:\n",
        "              inputs, targets = (b.to(device) for b in batch)\n",
        "\n",
        "              predictions = model(inputs)\n",
        "              loss = smooth_crossentropy(predictions, targets)\n",
        "              correct = torch.argmax(predictions, 1) == targets\n",
        "              log(model, loss.cpu(), correct.cpu())\n",
        "      if dp:\n",
        "          if not os.path.exists(MODEL_PATH + DATASET + 'dp/' + str(run)):\n",
        "              os.makedirs(MODEL_PATH + DATASET + 'dp/' + str(run))\n",
        "          torch.save(model, MODEL_PATH + DATASET + 'dp/' + str(run) + '/resnet_dp_0.005_' + str(epoch) + '.pt')\n",
        "      elif not SGD:\n",
        "          if not os.path.exists(MODEL_PATH + DATASET + 'sam/' + str(run)):\n",
        "              os.makedirs(MODEL_PATH + DATASET + 'sam/' + str(run))\n",
        "          torch.save(model, MODEL_PATH + DATASET + 'sam/' + str(run) + '/resnet_SAM_' + str(epoch) + '.pt')\n",
        "      elif dropout != 0:\n",
        "          if not os.path.exists(MODEL_PATH + DATASET + 'dropout/' + str(run)):\n",
        "              os.makedirs(MODEL_PATH + DATASET + 'dropout/' + str(run))\n",
        "          torch.save(model, MODEL_PATH + DATASET + 'dropout/' + str(run) + '/resnet_dropout_' + str(epoch) + '.pt')\n",
        "      elif weight_decay != 0.0005:\n",
        "          if not os.path.exists(MODEL_PATH + DATASET + 'l2_reg/' + str(run)):\n",
        "              os.makedirs(MODEL_PATH + DATASET + 'l2_reg/' + str(run))\n",
        "          torch.save(model, MODEL_PATH + DATASET + 'l2_reg/' + str(run) + '/resnet_l2_reg_' + str(epoch) + '.pt')\n",
        "      else:\n",
        "          if not os.path.exists(MODEL_PATH + DATASET + 'baseline/' + str(run)):\n",
        "                os.makedirs(MODEL_PATH + DATASET + 'baseline/' + str(run))\n",
        "          torch.save(model, MODEL_PATH + DATASET + 'baseline/' + str(run) + '/resnet_baseline_' + str(epoch) + '.pt')\n",
        "\n",
        "  log.flush()\n",
        "  return model"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JKv5lBQb3pP"
      },
      "source": [
        "class AttackModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AttackModel, self).__init__()\n",
        "        self.dout = nn.Dropout(0.2)\n",
        "        self.fc1 = nn.Linear(100, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.fc3 = nn.Linear(512, 256)\n",
        "        self.fc4 = nn.Linear(256, 2)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dout(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.fc4(x)\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "def get_attack_data(shadow_model, input_train, input_test, validate=False):\n",
        "  attack_x = None\n",
        "  for batch in input_train:\n",
        "    x, y = (b.to(device) for b in batch)\n",
        "    # print(x.shape)\n",
        "    pred = shadow_model(x).detach().cpu().numpy()\n",
        "    if attack_x is None:\n",
        "      attack_x = pred\n",
        "    else:\n",
        "      attack_x = np.r_[attack_x, pred]\n",
        "  m_train = np.ones(len(attack_x))\n",
        "  for batch in input_test:\n",
        "    x, y = (b.to(device) for b in batch)\n",
        "    # print(x.shape)\n",
        "    pred = shadow_model(x).detach().cpu().numpy()\n",
        "    if not validate:\n",
        "      # we will augment the data so that it will not be biased\n",
        "      attack_x = np.r_[attack_x, pred, pred, pred, pred, pred]\n",
        "    else:\n",
        "      attack_x = np.r_[attack_x, pred]\n",
        "  m_test = np.zeros(len(attack_x) - len(m_train))\n",
        "  return attack_x, np.r_[m_train, m_test]\n",
        "\n",
        "def train_attack_model(shadow_train, shadow_test):\n",
        "  # construct datasets for training attack models\n",
        "  attack_x, attack_y = get_attack_data(shadow_model, shadow_train, shadow_test)\n",
        "  attack_model = AttackModel()\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.SGD(attack_model.parameters(), lr=0.001, momentum=0.9)\n",
        "  attack_x = torch.Tensor(attack_x) # transform to torch tensor\n",
        "  attack_y = torch.Tensor(attack_y)\n",
        "  attack_m = attack_y.type(torch.LongTensor)\n",
        "  shokri_train = DataLoader(TensorDataset(attack_x, attack_m), shuffle=True) # create attack dataset\n",
        "\n",
        "  for epoch in range(10):\n",
        "      attack_model.train()\n",
        "      for i, data in enumerate(shokri_train):\n",
        "          # get the inputs; data is a list of [inputs, labels]\n",
        "          inputs, labels = data\n",
        "          # print(inputs[0], labels[0])\n",
        "\n",
        "          # zero the parameter gradients\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # forward + backward + optimize\n",
        "          outputs = attack_model(inputs)\n",
        "          # print(outputs, labels)\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "  print('Finished Training')\n",
        "  print('-'*10 + 'SUMMARY OF TRAIN ACC' + '*'*10)\n",
        "  predict_membership = model_evaluation(attack_model, attack_x, attack_y)\n",
        "  precision, recall, adversary_advantage = evaluation(predict_membership, attack_y.numpy())\n",
        "  print(\"Precision: \", precision)\n",
        "  print(\"Recall: \", recall)\n",
        "  print(\"Adv Advantage: \", adversary_advantage)\n",
        "  return attack_model\n",
        "\n",
        "# evaluate the performance of the model\n",
        "def model_evaluation(model, instances, membership):\n",
        "  instances = torch.Tensor(instances)\n",
        "  membership = torch.Tensor(membership)\n",
        "  membership = membership.type(torch.LongTensor)\n",
        "  dataloader = DataLoader(TensorDataset(instances, membership)) # create attack dataset\n",
        "  predict_membership = []\n",
        "  for i, data in enumerate(dataloader):\n",
        "    input, target = data\n",
        "    outputs = model(input)\n",
        "    predict_membership += list(torch.argmax(outputs, 1).numpy())\n",
        "  return predict_membership\n",
        "\n",
        "# perform the neural network attack\n",
        "def salem_membership_inference(target_model, shadow_train, shadow_test, validate_train, validate_test):\n",
        "  # shadow_train/shadow_test are data for training the attack model\n",
        "  # validate_train/validate_test are actual data to train the target model\n",
        "  attack_model = train_attack_model(shadow_train, shadow_test)\n",
        "  val_attack_x, val_attack_y = get_attack_data(target_model, validate_train, validate_test, validate=True)\n",
        "  # evaluate accuracy on actual membership inference\n",
        "  print('-'*10 + 'SUMMARY OF ATTACK' + '*'*10)\n",
        "  predict_membership = model_evaluation(attack_model, val_attack_x, val_attack_y)\n",
        "  precision, recall, adversary_advantage = evaluation(predict_membership, val_attack_y)\n",
        "  print(\"Precision: \", precision)\n",
        "  print(\"Recall: \", recall)\n",
        "  print(\"Adv Advantage: \", adversary_advantage)\n",
        "\n",
        "# return the precision, recall and adversary advantage\n",
        "def evaluation(pred_membership, true_membership):\n",
        "  tp, fp, tn, fn = 0, 0, 0, 0\n",
        "  for i in range(len(true_membership)):\n",
        "    if true_membership[i] == 1 and pred_membership[i] == 1:\n",
        "      tp += 1\n",
        "    elif true_membership[i] == 0 and pred_membership[i] == 1:\n",
        "      fp += 1\n",
        "    elif true_membership[i] == 1 and pred_membership[i] == 0:\n",
        "      fn += 1\n",
        "    else:\n",
        "      tn += 1\n",
        "  precision = tp / (tp + fp)\n",
        "  recall = tp / (tp + fn)\n",
        "  adversary_advantage = tp / (tp + fn) - fp / (tn + fp)\n",
        "  return precision, recall, adversary_advantage\n",
        "\n",
        "# yeom's attack, return the precision, recall and adversary advantage\n",
        "def yeom_attack(device, train, test, model):\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    train_correct = 0\n",
        "    correct = 0\n",
        "    total_train = 0\n",
        "    total_test = 0\n",
        "    total_loss = 0\n",
        "    correct_prediction = 0\n",
        "    yeom_prediction, per_instance_loss = [], []\n",
        "    tp, fp, tn, fn = 0, 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch in train:\n",
        "            # compute the avg loss\n",
        "            inputs, targets = (b.to(device) for b in batch)\n",
        "            pred = model(inputs)\n",
        "            loss = smooth_crossentropy(pred, targets)\n",
        "            total_loss += loss.sum().item()\n",
        "            total_train += targets.size(0)\n",
        "        total_loss /= total_train\n",
        "        # evaluate Yeom's attack\n",
        "        for batch in train:\n",
        "            # compute the avg loss\n",
        "            inputs, targets = (b.to(device) for b in batch)\n",
        "            pred = model(inputs)\n",
        "            _, predicted = torch.max(pred, 1)\n",
        "            loss = smooth_crossentropy(pred, targets)\n",
        "            # append the per instance loss\n",
        "            per_instance_loss += list(loss.detach().cpu().numpy())\n",
        "            train_correct += (predicted == targets).sum().item()\n",
        "            yeom_prediction += list((loss <= total_loss).detach().cpu().numpy() + 0)\n",
        "        # print(\"This is the length\", len(yeom_prediction))\n",
        "        for batch in test:\n",
        "            inputs, targets = (b.to(device) for b in batch)\n",
        "            pred = model(inputs)\n",
        "            _, predicted = torch.max(pred, 1)\n",
        "            loss = smooth_crossentropy(pred, targets)\n",
        "            # append the per instance loss\n",
        "            per_instance_loss += list(loss.detach().cpu().numpy())\n",
        "            total_test += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "            yeom_prediction += list((loss <= total_loss).detach().cpu().numpy() + 0)\n",
        "        # print(inputs, targets)\n",
        "    # print(per_instance_loss)\n",
        "    yeom_prediction = np.array(yeom_prediction)\n",
        "    per_instance_loss = np.array(per_instance_loss)\n",
        "    tp, fn = np.sum(yeom_prediction[:total_train]), total_train - np.sum(yeom_prediction[:total_train])\n",
        "    fp, tn = np.sum(yeom_prediction[total_train:]), total_test - np.sum(yeom_prediction[total_train:])\n",
        "    print(\"-\" * 10 + \"Evaluation for Yeom's attack\" + \"-\" * 10)\n",
        "    print(\"The Test Accuracy is \", correct / total_test)\n",
        "    print(\"True Positive: \", tp, \"      True Negative: \", tn, \"     False Positive: \", fp, \"     False Negative: \", fn)\n",
        "    print(\"Precision: \", tp / (tp + fp), \"      Recall:\", tp / (tp + fn))\n",
        "    yeom_advantage = tp / (tp + fn) - fp / (tn + fp)\n",
        "    print(\"Adversary Advantage: \", yeom_advantage)\n",
        "\n",
        "    # print(\"-\" * 10 + \"Evaluation for Balanced Training and Testing sets\" + \"-\" * 10)\n",
        "    # random_index = np.random.choice(yeom_prediction[:total_test].shape[0], total_test, replace=False)\n",
        "    # tp, fn = np.sum(yeom_prediction[random_index]), total_test - np.sum(yeom_prediction[random_index])\n",
        "    # train_acc = train_correct / total_train\n",
        "    # test_acc = correct / total_test\n",
        "    # target_membership = np.concatenate([np.ones(total_train), np.zeros(total_test)])\n",
        "    # yeom_advantage = tp / (tp + fn) - fp / (tn + fp)\n",
        "\n",
        "    # print(\"The Test Accuracy is \", correct / total_test)\n",
        "    # print(\"True Positive: \", tp, \"      True Negative: \", tn, \"     False Positive: \", fp, \"     False Negative: \", fn)\n",
        "    # print(\"Precision: \", tp / (tp + fp), \"      Recall:\", tp / (tp + fn))\n",
        "    # res = [train_acc, test_acc, total_loss, target_membership, yeom_prediction, per_instance_loss, yeom_advantage]\n",
        "    # return res\n",
        "\n",
        "# the whole process, with attack models saved\n",
        "# run is the index of run we are currently on\n",
        "def run_experiment(run, save_data=False, num_splits=2, batch_size=32, virtual_batch_size=256, dp=False,\n",
        "                   pretrained=True, learning_rate=0.1, SGD=True, dropout=0, l2_reg=0.0005):\n",
        "  if save_data:\n",
        "      mean, std = (0.5074, 0.4867, 0.4411), (0.2675, 0.2566, 0.2763)\n",
        "\n",
        "      transform = transforms.Compose([\n",
        "          transforms.Resize(224),\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize(mean, std)\n",
        "      ])\n",
        "\n",
        "      train_set = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "      test_set = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "      train_sets = split_dataset(train_set, num_splits, batch_size)\n",
        "      test_sets = split_dataset(test_set, num_splits, batch_size)\n",
        "      for i in range(num_splits):\n",
        "        torch.save(train_sets[i], 'train' + str(i) + '.pt')\n",
        "        torch.save(test_sets[i], 'test' + str(i) + '.pt')\n",
        "  else:\n",
        "      train_x = torch.load('train0.pt')\n",
        "      test_x = torch.load('test0.pt')\n",
        "      model = train_target_model(train_x, test_x, run, pretrained=pretrained, dp=dp, SGD=SGD,\n",
        "                                 dropout=dropout, learning_rate=learning_rate, weight_decay=l2_reg)\n",
        "      if dp:\n",
        "          torch.save(model, MODEL_PATH + DATASET + 'resnet_dp_0.005.pt')\n",
        "      elif SAM:\n",
        "          torch.save(model, MODEL_PATH + DATASET + 'resnet_SAM.pt')\n",
        "      elif dropout != 0:\n",
        "          torch.save(model, MODEL_PATH + DATASET + 'resnet_dropout.pt')\n",
        "      elif reg != 0.0005:\n",
        "          torch.save(model, MODEL_PATH + DATASET + 'resnet_l2_reg.pt')\n",
        "      else:\n",
        "          torch.save(model, MODEL_PATH + DATASET + 'resnet_baseline.pt')\n",
        "  return"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gsXUJdnaN2l"
      },
      "source": [
        "# run_experiment(save_data=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJe3lt7zan-4",
        "outputId": "47380e90-4b3b-4712-8091-122aa4b92c0b"
      },
      "source": [
        "# run experiment to generate target models\n",
        "for i in range(1):\n",
        "  run_experiment(i+1, learning_rate=8e-3)\n",
        "  run_experiment(i+1, dropout=0.5, learning_rate=8e-3)\n",
        "  run_experiment(i+1, l2_reg=0.0025, learning_rate=8e-3)\n",
        "  run_experiment(i+1, SGD=False, learning_rate=8e-3)\n",
        "  run_experiment(i+1, dp=True, learning_rate=8e-3)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "┏━━━━━━━━━━━━━━┳━━━━━━━╸T╺╸R╺╸A╺╸I╺╸N╺━━━━━━━┳━━━━━━━╸S╺╸T╺╸A╺╸T╺╸S╺━━━━━━━┳━━━━━━━╸V╺╸A╺╸L╺╸I╺╸D╺━━━━━━━┓\n",
            "┃              ┃              ╷              ┃              ╷              ┃              ╷              ┃\n",
            "┃       epoch  ┃        loss  │    accuracy  ┃        l.r.  │     elapsed  ┃        loss  │    accuracy  ┃\n",
            "┠──────────────╂──────────────┼──────────────╂──────────────┼──────────────╂──────────────┼──────────────┨\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:523: UserWarning: A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            "  \"A ``sample_rate`` has been provided.\"\n",
            "/usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:195: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            "  \"Secure RNG turned off. This is perfectly fine for experimentation as it allows \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "┃           0  ┃      1.8583  │     49.93 %  ┃   8.000e-03  │   02:15 min  ┃      1.3823  │     60.44 %  ┃\n",
            "┃           1  ┃      1.0800  │     67.62 %  ┃   8.000e-03  │   02:15 min  ┃      1.2910  │     65.42 %  ┃\n",
            "┃           2  ┃      0.8214  │     74.88 %  ┃   8.000e-03  │   02:15 min  ┃      1.4744  │     64.86 %  ┃\n",
            "┃           3  ┃      0.6467  │     79.78 %  ┃   8.000e-03  │   02:15 min  ┃      1.4890  │     65.06 %  ┃\n",
            "┃           4  ┃      0.5304  │     83.39 %  ┃   8.000e-03  │   02:15 min  ┃      1.4633  │     68.10 %  ┃\n",
            "┃           5  ┃      0.1995  │     93.84 %  ┃   1.600e-03  │   02:15 min  ┃      1.2603  │     79.60 %  ┃\n",
            "┃           6  ┃      0.0852  │     97.80 %  ┃   1.600e-03  │   02:15 min  ┃      1.3152  │     80.30 %  ┃\n",
            "┃           7  ┃      0.0480  │     99.11 %  ┃   1.600e-03  │   02:15 min  ┃      1.3509  │     80.40 %  ┃\n",
            "┃           8  ┃      0.0312  │     99.52 %  ┃   1.600e-03  │   02:15 min  ┃      1.3784  │     80.12 %  ┃\n",
            "┃           9  ┃      0.0227  │     99.71 %  ┃   3.200e-04  │   02:15 min  ┃      1.3760  │     80.34 %  ┃\n",
            "┃          10  ┃      0.0206  │     99.75 %  ┃   3.200e-04  │   02:15 min  ┃      1.3815  │     80.42 %  ┃\n",
            "┃          11  ┃      0.0192  │     99.77 %  ┃   3.200e-04  │   02:15 min  ┃      1.3865  │     80.42 %  ┃\n",
            "┃          12  ┃      0.0180  │     99.78 %  ┃   6.400e-05  │   02:15 min  ┃      1.3850  │     80.36 %  ┃\n",
            "┃          13  ┃      0.0176  │     99.79 %  ┃   6.400e-05  │   02:15 min  ┃      1.3856  │     80.34 %  ┃\n",
            "┃          14  ┃      0.0173  │     99.80 %  ┃   6.400e-05  │   02:15 min  ┃      1.3865  │     80.34 %  ┃\n",
            "┏━━━━━━━━━━━━━━┳━━━━━━━╸T╺╸R╺╸A╺╸I╺╸N╺━━━━━━━┳━━━━━━━╸S╺╸T╺╸A╺╸T╺╸S╺━━━━━━━┳━━━━━━━╸V╺╸A╺╸L╺╸I╺╸D╺━━━━━━━┓\n",
            "┃              ┃              ╷              ┃              ╷              ┃              ╷              ┃\n",
            "┃       epoch  ┃        loss  │    accuracy  ┃        l.r.  │     elapsed  ┃        loss  │    accuracy  ┃\n",
            "┠──────────────╂──────────────┼──────────────╂──────────────┼──────────────╂──────────────┼──────────────┨\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:523: UserWarning: A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            "  \"A ``sample_rate`` has been provided.\"\n",
            "/usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:195: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            "  \"Secure RNG turned off. This is perfectly fine for experimentation as it allows \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "┃           0  ┃      2.2509  │     40.95 %  ┃   8.000e-03  │   02:15 min  ┃      1.3831  │     58.74 %  ┃\n",
            "┃           1  ┃      1.3992  │     59.91 %  ┃   8.000e-03  │   02:15 min  ┃      1.3084  │     62.12 %  ┃\n",
            "┃           2  ┃      1.1155  │     67.14 %  ┃   8.000e-03  │   02:15 min  ┃      1.3169  │     64.98 %  ┃\n",
            "┃           3  ┃      0.9434  │     71.78 %  ┃   8.000e-03  │   02:15 min  ┃      1.3471  │     66.10 %  ┃\n",
            "┃           4  ┃      0.8159  │     75.06 %  ┃   8.000e-03  │   02:15 min  ┃      1.5484  │     63.18 %  ┃\n",
            "┃           5  ┃      0.3898  │     87.53 %  ┃   1.600e-03  │   02:15 min  ┃      1.2343  │     77.88 %  ┃\n",
            "┃           6  ┃      0.2411  │     92.16 %  ┃   1.600e-03  │   02:15 min  ┃      1.3339  │     78.24 %  ┃\n",
            "┃           7  ┃      0.1700  │     94.67 %  ┃   1.600e-03  │   02:15 min  ┃      1.3774  │     78.40 %  ┃\n",
            "┃           8  ┃      0.1232  │     96.22 %  ┃   1.600e-03  │   02:15 min  ┃      1.4207  │     78.88 %  ┃\n",
            "┃           9  ┃      0.0864  │     97.50 %  ┃   3.200e-04  │   02:15 min  ┃      1.4140  │     79.64 %  ┃\n",
            "┃          10  ┃      0.0719  │     98.09 %  ┃   3.200e-04  │   02:15 min  ┃      1.4279  │     79.78 %  ┃\n",
            "┃          11  ┃      0.0639  │     98.33 %  ┃   3.200e-04  │   02:15 min  ┃      1.4453  │     79.44 %  ┃\n",
            "┃          12  ┃      0.0600  │     98.37 %  ┃   6.400e-05  │   02:15 min  ┃      1.4455  │     79.54 %  ┃\n",
            "┃          13  ┃      0.0573  │     98.59 %  ┃   6.400e-05  │   02:15 min  ┃      1.4504  │     79.64 %  ┃\n",
            "┃          14  ┃      0.0541  │     98.71 %  ┃   6.400e-05  │   02:15 min  ┃      1.4524  │     79.68 %  ┃\n",
            "┏━━━━━━━━━━━━━━┳━━━━━━━╸T╺╸R╺╸A╺╸I╺╸N╺━━━━━━━┳━━━━━━━╸S╺╸T╺╸A╺╸T╺╸S╺━━━━━━━┳━━━━━━━╸V╺╸A╺╸L╺╸I╺╸D╺━━━━━━━┓\n",
            "┃              ┃              ╷              ┃              ╷              ┃              ╷              ┃\n",
            "┃       epoch  ┃        loss  │    accuracy  ┃        l.r.  │     elapsed  ┃        loss  │    accuracy  ┃\n",
            "┠──────────────╂──────────────┼──────────────╂──────────────┼──────────────╂──────────────┼──────────────┨\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:523: UserWarning: A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            "  \"A ``sample_rate`` has been provided.\"\n",
            "/usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:195: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            "  \"Secure RNG turned off. This is perfectly fine for experimentation as it allows \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "┃           0  ┃      1.8970  │     48.62 %  ┃   8.000e-03  │   02:15 min  ┃      1.3680  │     60.10 %  ┃\n",
            "┃           1  ┃      1.1377  │     66.25 %  ┃   8.000e-03  │   02:15 min  ┃      1.2711  │     64.68 %  ┃\n",
            "┃           2  ┃      0.9506  │     71.37 %  ┃   8.000e-03  │   02:15 min  ┃      1.3703  │     64.50 %  ┃\n",
            "┃           3  ┃      0.8388  │     74.49 %  ┃   8.000e-03  │   02:15 min  ┃      1.3546  │     65.36 %  ┃\n",
            "┃           4  ┃      0.7650  │     76.47 %  ┃   8.000e-03  │   02:15 min  ┃      1.5209  │     60.76 %  ┃\n",
            "┃           5  ┃      0.3226  │     90.08 %  ┃   1.600e-03  │   02:15 min  ┃      1.0542  │     77.52 %  ┃\n",
            "┃           6  ┃      0.1579  │     95.97 %  ┃   1.600e-03  │   02:15 min  ┃      1.0695  │     78.64 %  ┃\n",
            "┃           7  ┃      0.0934  │     98.24 %  ┃   1.600e-03  │   02:15 min  ┃      1.0774  │     79.12 %  ┃\n",
            "┃           8  ┃      0.0597  │     99.10 %  ┃   1.600e-03  │   02:15 min  ┃      1.0859  │     79.00 %  ┃\n",
            "┃           9  ┃      0.0410  │     99.50 %  ┃   3.200e-04  │   02:15 min  ┃      1.0791  │     79.54 %  ┃\n",
            "┃          10  ┃      0.0361  │     99.60 %  ┃   3.200e-04  │   02:15 min  ┃      1.0804  │     79.50 %  ┃\n",
            "┃          11  ┃      0.0330  │     99.65 %  ┃   3.200e-04  │   02:15 min  ┃      1.0818  │     79.66 %  ┃\n",
            "┃          12  ┃      0.0307  │     99.69 %  ┃   6.400e-05  │   02:15 min  ┃      1.0803  │     79.76 %  ┃\n",
            "┃          13  ┃      0.0297  │     99.72 %  ┃   6.400e-05  │   02:15 min  ┃      1.0807  │     79.62 %  ┃\n",
            "┃          14  ┃      0.0292  │     99.74 %  ┃   6.400e-05  │   02:15 min  ┃      1.0811  │     79.62 %  ┃\n",
            "┏━━━━━━━━━━━━━━┳━━━━━━━╸T╺╸R╺╸A╺╸I╺╸N╺━━━━━━━┳━━━━━━━╸S╺╸T╺╸A╺╸T╺╸S╺━━━━━━━┳━━━━━━━╸V╺╸A╺╸L╺╸I╺╸D╺━━━━━━━┓\n",
            "┃              ┃              ╷              ┃              ╷              ┃              ╷              ┃\n",
            "┃       epoch  ┃        loss  │    accuracy  ┃        l.r.  │     elapsed  ┃        loss  │    accuracy  ┃\n",
            "┠──────────────╂──────────────┼──────────────╂──────────────┼──────────────╂──────────────┼──────────────┨\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:523: UserWarning: A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            "  \"A ``sample_rate`` has been provided.\"\n",
            "/usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:195: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            "  \"Secure RNG turned off. This is perfectly fine for experimentation as it allows \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "┃           0  ┃      1.5441  │     53.11 %  ┃   8.000e-03  │   04:23 min  ┃      1.0780  │     65.80 %  ┃\n",
            "┃           1  ┃      0.8253  │     74.16 %  ┃   8.000e-03  │   04:23 min  ┃      0.8945  │     71.58 %  ┃\n",
            "┃           2  ┃      0.6297  │     81.46 %  ┃   8.000e-03  │   04:23 min  ┃      0.8495  │     72.54 %  ┃\n",
            "┃           3  ┃      0.4951  │     86.70 %  ┃   8.000e-03  │   04:24 min  ┃      0.8444  │     73.00 %  ┃\n",
            "┃           4  ┃      0.4016  │     90.06 %  ┃   8.000e-03  │   04:23 min  ┃      0.7919  │     74.80 %  ┃\n",
            "┃           5  ┃      0.2425  │     95.46 %  ┃   1.600e-03  │   04:23 min  ┃      0.6043  │     81.66 %  ┃\n",
            "┃           6  ┃      0.1845  │     97.27 %  ┃   1.600e-03  │   04:23 min  ┃      0.5921  │     82.04 %  ┃\n",
            "┃           7  ┃      0.1537  │     98.08 %  ┃   1.600e-03  │   04:23 min  ┃      0.5892  │     82.14 %  ┃\n",
            "┃           8  ┃      0.1313  │     98.63 %  ┃   1.600e-03  │   04:24 min  ┃      0.5898  │     81.92 %  ┃\n",
            "┃           9  ┃      0.1146  │     98.96 %  ┃   3.200e-04  │   04:23 min  ┃      0.5815  │     82.54 %  ┃\n",
            "┃          10  ┃      0.1101  │     99.03 %  ┃   3.200e-04  │   04:23 min  ┃      0.5816  │     82.36 %  ┃\n",
            "┃          11  ┃      0.1067  │     99.08 %  ┃   3.200e-04  │   04:23 min  ┃      0.5820  │     82.28 %  ┃\n",
            "┃          12  ┃      0.1038  │     99.11 %  ┃   6.400e-05  │   04:23 min  ┃      0.5778  │     82.26 %  ┃\n",
            "┃          13  ┃      0.1027  │     99.16 %  ┃   6.400e-05  │   04:23 min  ┃      0.5778  │     82.24 %  ┃\n",
            "┃          14  ┃      0.1020  │     99.16 %  ┃   6.400e-05  │   04:23 min  ┃      0.5780  │     82.28 %  ┃\n",
            "┏━━━━━━━━━━━━━━┳━━━━━━━╸T╺╸R╺╸A╺╸I╺╸N╺━━━━━━━┳━━━━━━━╸S╺╸T╺╸A╺╸T╺╸S╺━━━━━━━┳━━━━━━━╸V╺╸A╺╸L╺╸I╺╸D╺━━━━━━━┓\n",
            "┃              ┃              ╷              ┃              ╷              ┃              ╷              ┃\n",
            "┃       epoch  ┃        loss  │    accuracy  ┃        l.r.  │     elapsed  ┃        loss  │    accuracy  ┃\n",
            "┠──────────────╂──────────────┼──────────────╂──────────────┼──────────────╂──────────────┼──────────────┨\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:523: UserWarning: A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            "  \"A ``sample_rate`` has been provided.\"\n",
            "/usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:195: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            "  \"Secure RNG turned off. This is perfectly fine for experimentation as it allows \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "┃           0  ┃      4.3199  │     10.03 %  ┃   8.000e-03  │   04:35 min  ┃      3.1865  │     21.62 %  ┃\n",
            "┃           1  ┃      3.3953  │     30.10 %  ┃   8.000e-03  │   04:35 min  ┃      2.3967  │     35.34 %  ┃\n",
            "┃           2  ┃      2.6067  │     41.29 %  ┃   8.000e-03  │   04:35 min  ┃      1.9076  │     44.32 %  ┃\n",
            "┃           3  ┃      2.1317  │     48.48 %  ┃   8.000e-03  │   04:35 min  ┃      1.6465  │     49.18 %  ┃\n",
            "┃           4  ┃      1.8390  │     53.30 %  ┃   8.000e-03  │   04:35 min  ┃      1.4922  │     53.46 %  ┃\n",
            "┃           5  ┃      1.6829  │     56.55 %  ┃   1.600e-03  │   04:35 min  ┃      1.4496  │     55.14 %  ┃\n",
            "┃           6  ┃      1.6380  │     57.89 %  ┃   1.600e-03  │   04:35 min  ┃      1.4263  │     55.78 %  ┃\n",
            "┃           7  ┃      1.6022  │     58.69 %  ┃   1.600e-03  │   04:35 min  ┃      1.4063  │     56.40 %  ┃\n",
            "┃           8  ┃      1.5693  │     59.22 %  ┃   1.600e-03  │   04:35 min  ┃      1.3883  │     56.92 %  ┃\n",
            "┃           9  ┃      1.5454  │     59.66 %  ┃   3.200e-04  │   04:35 min  ┃      1.3856  │     57.08 %  ┃\n",
            "┃          10  ┃      1.5395  │     59.82 %  ┃   3.200e-04  │   04:35 min  ┃      1.3820  │     57.10 %  ┃\n",
            "┃          11  ┃      1.5334  │     59.91 %  ┃   3.200e-04  │   04:35 min  ┃      1.3786  │     57.16 %  ┃\n",
            "┃          12  ┃      1.5294  │     60.01 %  ┃   6.400e-05  │   04:35 min  ┃      1.3775  │     57.20 %  ┃\n",
            "┃          13  ┃      1.5279  │     60.03 %  ┃   6.400e-05  │   04:35 min  ┃      1.3766  │     57.26 %  ┃\n",
            "┃          14  ┃      1.5266  │     60.03 %  ┃   6.400e-05  │   04:35 min  ┃      1.3758  │     57.28 %  ┃\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YurrRUs_6gmP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "senu_OisbKhE",
        "outputId": "f7c7cb19-9876-4725-c6aa-c00e8948583b"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "target_model = torch.load('model0.pt').to(device)\n",
        "shadow_model = torch.load('model1.pt').to(device)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-543528a94afc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtarget_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model0.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mshadow_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model1.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model0.pt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMYQWkMD0eQH"
      },
      "source": [
        "shadow_train = torch.load('train1.pt')\n",
        "shadow_test = torch.load('test1.pt')\n",
        "validate_train = torch.load('train0.pt')\n",
        "validate_test = torch.load('test0.pt')\n",
        "salem_membership_inference(target_model, shadow_train, shadow_test, validate_train, validate_test)\n",
        "yeom_attack(device, validate_train, validate_test, target_model)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}